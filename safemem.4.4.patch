diff -urN linux-4.4/include/linux/mm_types.h linux/include/linux/mm_types.h
--- linux-4.4/include/linux/mm_types.h	2016-01-11 07:01:32.000000000 +0800
+++ linux/include/linux/mm_types.h	2016-12-08 10:05:25.275407028 +0800
@@ -14,6 +14,7 @@
 #include <linux/page-flags-layout.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
+#include <linux/time.h>
 
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
@@ -218,6 +219,10 @@
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
+
+#ifdef CONFIG_SAFEMEM
+	struct timeval last_test_time;
+#endif
 }
 /*
  * The struct page can be forced to be double word aligned so that atomic ops
diff -urN linux-4.4/include/linux/mmzone.h linux/include/linux/mmzone.h
--- linux-4.4/include/linux/mmzone.h	2016-01-11 07:01:32.000000000 +0800
+++ linux/include/linux/mmzone.h	2016-12-08 10:05:25.241406308 +0800
@@ -478,6 +478,13 @@
 	/* free areas of different sizes */
 	struct free_area	free_area[MAX_ORDER];
 
+#ifdef CONFIG_SAFEMEM
+	/* safe_list is used to collect single pages that have been tested recently,
+	 * to guarantee that these pages do not have hardware errors.
+	 */
+	struct list_head safe_list[MIGRATE_TYPES];
+	int safe_count;
+#endif
 	/* zone flags, see below */
 	unsigned long		flags;
 
diff -urN linux-4.4/init/Kconfig linux/init/Kconfig
--- linux-4.4/init/Kconfig	2016-01-11 07:01:32.000000000 +0800
+++ linux/init/Kconfig	2016-12-08 10:05:27.220448180 +0800
@@ -28,6 +28,12 @@
 
 menu "General setup"
 
+config SAFEMEM
+	bool "Enable SAFEMEM"
+	help
+	  Enable the applications' safe memory mode to get rid of
+	  the effect of hardware errors.
+
 config BROKEN
 	bool
 
diff -urN linux-4.4/mm/migrate.c linux/mm/migrate.c
--- linux-4.4/mm/migrate.c	2016-01-11 07:01:32.000000000 +0800
+++ linux/mm/migrate.c	2016-12-08 10:05:49.700923813 +0800
@@ -63,6 +63,7 @@
 
 	return 0;
 }
+EXPORT_SYMBOL(migrate_prep);
 
 /* Do the necessary work of migrate_prep but not if it involves other CPUs */
 int migrate_prep_local(void)
@@ -1185,6 +1186,7 @@
 
 	return rc;
 }
+EXPORT_SYMBOL(migrate_pages);
 
 #ifdef CONFIG_NUMA
 /*
diff -urN linux-4.4/mm/mmzone.c linux/mm/mmzone.c
--- linux-4.4/mm/mmzone.c	2016-01-11 07:01:32.000000000 +0800
+++ linux/mm/mmzone.c	2016-12-08 10:05:46.796862370 +0800
@@ -13,6 +13,7 @@
 {
 	return NODE_DATA(first_online_node);
 }
+EXPORT_SYMBOL(first_online_pgdat);
 
 struct pglist_data *next_online_pgdat(struct pglist_data *pgdat)
 {
@@ -41,6 +42,7 @@
 	}
 	return zone;
 }
+EXPORT_SYMBOL(next_zone);
 
 static inline int zref_in_nodemask(struct zoneref *zref, nodemask_t *nodes)
 {
diff -urN linux-4.4/mm/page_alloc.c linux/mm/page_alloc.c
--- linux-4.4/mm/page_alloc.c	2016-01-11 07:01:32.000000000 +0800
+++ linux/mm/page_alloc.c	2016-12-08 10:05:47.441876017 +0800
@@ -68,6 +68,14 @@
 #include <asm/div64.h>
 #include "internal.h"
 
+#include <linux/debugfs.h>
+
+#ifdef CONFIG_SAFEMEM
+/* the switch of the SAFEMEM */
+u32 use_appsafemem = 0;
+EXPORT_SYMBOL(use_appsafemem);
+#endif
+
 /* prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */
 static DEFINE_MUTEX(pcp_batch_high_lock);
 #define MIN_PERCPU_PAGELIST_FRACTION	(8)
@@ -653,7 +661,7 @@
  * -- nyc
  */
 
-static inline void __free_one_page(struct page *page,
+inline void __free_one_page(struct page *page,
 		unsigned long pfn,
 		struct zone *zone, unsigned int order,
 		int migratetype)
@@ -733,6 +741,7 @@
 out:
 	zone->free_area[order].nr_free++;
 }
+EXPORT_SYMBOL(__free_one_page);
 
 static inline int free_pages_check(struct page *page)
 {
@@ -1402,10 +1411,161 @@
 	return 0;
 }
 
+#ifdef CONFIG_SAFEMEM
+
+/* The hook to test page
+ * 0 - page is ok
+ * 1 - page is bad
+ */
+static int (*test_page)(struct page *page);
+
+void register_test_page(int (*fn)(struct page *page))
+{
+	test_page = fn;
+}
+EXPORT_SYMBOL(register_test_page);
+
+void unregister_test_page(void)
+{
+	test_page = NULL;
+}
+EXPORT_SYMBOL(unregister_test_page);
+
+static struct page *safemem_alloc(struct zone *zone, int migratetype)
+{
+	struct page *page = NULL;
+
+	if (list_empty(&(zone->safe_list[migratetype])))
+		return NULL;
+
+	/* TODO: use the statistics of safe_list number */
+//	if (!list_empty(&(zone->safe_list[migratetype])))
+	page = list_entry(zone->safe_list[migratetype].next, struct page, lru);
+	if (PageReserved(page))
+		return NULL;
+
+	/* remove from the safe_list */
+	list_del(&page->lru);
+	zone->free_area[0].nr_free--;
+	zone->safe_count--;
+//	pr_info("safemem: zone %s: safe_count: %d\n", zone->name, zone->safe_count);
+	
+	return page;
+}
+
+static void safemem_insert(struct zone *zone, struct page *page, int order, int migratetype)
+{
+	int i;
+	int ret = 0;
+
+	if (PageReserved(page))
+		return ;
+
+	list_del(&page->lru);
+	zone->free_area[order].nr_free--;
+
+	for (i = 0; i < (1<<order); i++) {
+		if (PageReserved(&page[i]))
+			continue;
+		if (test_page)
+			ret = test_page(&page[i]);
+		if (ret)
+			continue;
+		INIT_LIST_HEAD(&page[i].lru);
+		list_add_tail(&page[i].lru, &zone->safe_list[migratetype]);
+		zone->free_area[0].nr_free++;
+		zone->safe_count++;
+//		pr_info("safemem: zone %s: safe_count: %d\n", zone->name, zone->safe_count);
+		rmv_page_order(&page[i]);
+	}
+}
+
+void appsafemem_flush(struct zone *zone)
+{
+	struct page *page;
+	int i = 0;
+
+	for (i = MIGRATE_UNMOVABLE; i < MIGRATE_TYPES; i++) {
+		while (!list_empty(&zone->safe_list[i])) {
+			page = list_entry(zone->safe_list[i].next, struct page, lru);
+			list_del_init(&page->lru);
+			__free_one_page(page, page_to_pfn(page), zone, 0, get_pageblock_migratetype(page));
+			zone->free_area[0].nr_free--;
+			zone->safe_count--;
+		}
+		INIT_LIST_HEAD(&zone->safe_list[i]);
+	}
+}
+EXPORT_SYMBOL(appsafemem_flush);
+
+#endif
+
 /*
  * Go through the free lists for the given migratetype and remove
  * the smallest available page from the freelists
  */
+#ifdef CONFIG_SAFEMEM
+
+static inline
+struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
+						int migratetype)
+{
+	unsigned int current_order;
+	struct free_area *area;
+	struct page *page = NULL;
+	struct list_head *curr, *tmp;
+
+	if (!use_appsafemem)
+		goto normal_buddy_alloc;
+
+	if (order == 0) {
+		/* TODO: use migratetye parameter */
+		page = safemem_alloc(zone, migratetype);
+		if (page)
+			return page;
+	}
+
+	/* not allocation successful in previous allocation, supply the
+	 * safe_list.
+	 */
+	if (order == 0) {
+		for (current_order = 0; current_order < MAX_ORDER; ++current_order) {
+			area = &(zone->free_area[current_order]);
+			if (list_empty(&area->free_list[migratetype]))
+				continue;
+			
+			list_for_each_safe(curr, tmp, &area->free_list[migratetype]) {
+				page = list_entry(curr, struct page, lru);
+				safemem_insert(zone, page, current_order, migratetype);
+				page = safemem_alloc(zone, migratetype);
+
+				if (page)
+					return page;
+			}
+		}
+	} else {
+normal_buddy_alloc:
+		/* Find a page of the appropriate size in the preferred list */
+		for (current_order = order; current_order < MAX_ORDER; ++current_order) {
+			area = &(zone->free_area[current_order]);
+			if (list_empty(&area->free_list[migratetype]))
+				continue;
+
+			page = list_entry(area->free_list[migratetype].next,
+								struct page, lru);
+			list_del(&page->lru);
+			rmv_page_order(page);
+			area->nr_free--;
+			expand(zone, page, order, current_order, area, migratetype);
+			set_pcppage_migratetype(page, migratetype);
+			return page;
+		}
+	}
+	return NULL;
+}
+
+#else /* !CONFIG_SAFEMEM */
+
 static inline
 struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
 						int migratetype)
@@ -1433,6 +1593,7 @@
 	return NULL;
 }
 
+#endif
 
 /*
  * This array describes the order lists are fallen back to when
@@ -2196,7 +2357,12 @@
 	struct page *page;
 	bool cold = ((gfp_flags & __GFP_COLD) != 0);
 
+/* skip pcp, since some pages of pcp may not be tested */
+#ifdef CONFIG_SAFEMEM
+	if (likely(order == 0) && !use_appsafemem) {
+#else /* !CONFIG_SAFEMEM */
 	if (likely(order == 0)) {
+#endif
 		struct per_cpu_pages *pcp;
 		struct list_head *list;
 
@@ -4520,6 +4686,15 @@
 static void __meminit zone_init_free_lists(struct zone *zone)
 {
 	unsigned int order, t;
+	int i = 0;
+
+#ifdef CONFIG_SAFEMEM
+	for (i = MIGRATE_UNMOVABLE; i < MIGRATE_TYPES; i++) {
+		INIT_LIST_HEAD(&zone->safe_list[i]);
+	}
+	zone->safe_count = 0;
+#endif
+
 	for_each_migratetype_order(order, t) {
 		INIT_LIST_HEAD(&zone->free_area[order].free_list[t]);
 		zone->free_area[order].nr_free = 0;
@@ -6451,6 +6626,7 @@
 	bitidx += end_bitidx;
 	return (word >> (BITS_PER_LONG - bitidx - 1)) & mask;
 }
+EXPORT_SYMBOL(get_pfnblock_flags_mask);
 
 /**
  * set_pfnblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages
@@ -6854,6 +7030,9 @@
 		return;
 	zone = page_zone(pfn_to_page(pfn));
 	spin_lock_irqsave(&zone->lock, flags);
+#ifdef CONFIG_SAFEMEM
+	appsafemem_flush(zone);
+#endif
 	pfn = start_pfn;
 	while (pfn < end_pfn) {
 		if (!pfn_valid(pfn)) {
diff -urN linux-4.4/mm/vmscan.c linux/mm/vmscan.c
--- linux-4.4/mm/vmscan.c	2016-01-11 07:01:32.000000000 +0800
+++ linux/mm/vmscan.c	2016-12-08 10:05:47.145869754 +0800
@@ -1444,6 +1444,7 @@
 	}
 	return ret;
 }
+EXPORT_SYMBOL(isolate_lru_page);
 
 /*
  * A direct reclaimer may isolate SWAP_CLUSTER_MAX pages from the LRU list and
